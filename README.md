# mRNA-nuclear-export

## Step 1: RNAseq processing and QC
All files/scripts related to this step are listed under the `RNAseq processing pipeline` folder.  
RNAseq data were processed from qseq to counts using the pipeline `pipeline_ENCODE_ref.sh` ran on a Linux server ubuntu 16.04. The ouptut of this step are count files which are made available in the `Data > merged_counts` folder. We recommend starting after this step.  
For this step multiple scripts were used for each processing substeps:
* Conversion of qseq to fastq files see bash script`02_consolidated_michael.sh`
* Trimming adapter sequence using cudadapt, to see exact options used see `03_trimming_index_universal.sh`. Fastq files obtained after this step were deposit on ENCODE
* Reads were aligned using STAR software, to see exact options used see `05_trimming_index_universal.sh`
* Alignments were filtered to keep only reads mapped in proper pair and remove unmapped read/mates or failing vendor quality using samtools, to see exact options used see `06_filtering.sh`. 
* Alignment were filtered to only keep uniquely aligned reads using samtools. To see exact options used see `06_filtering_unique.sh`.
* Optional. Create bigwig track files to visualize sequencing with IGV using RSeQC `bamtowig.py` and USCS `wigToBigWig` tools, for exact options see `07_tracks.sh` and `07_tracks_unique.sh`
* Counts were generated using featureCounts. To see exact options used see `08_counts.sh`. Count files obtained from this step are available in the folder `Data > individual_counts`. 
* QC of the various steps was visualized using multiQC. Additionaly PCA using R was also done on all the technical replicates, see `QC > PCA.R`
* Bam files corresponding to technical replicates were merged into a single bam file using samtools, see `merging_bam.sh` for exact options. Bams files generated by this step were deposited on ENCODE. Counts corresponding to merged bam files were generated using featureCounts. To see exact options used see `merged_counts.sh`. Count files obtained from this step are available in the folder `Data > merged_counts`. 

### Requirements for Step 1
Needed external softwares:
* python
* cutadapt
* STAR
* samtools
* RSeQC
* wigToBigWig
* multiQC
* R (ggplot2, edgeR)

## Step 2: Get List of early Lipid A inducible genes
This step starts by creating R datasets of rpkm from the raw counts in `Data > merged_counts` using the script `Data > Create Datasets.R`.
This script must be run from within the `Data` folder. This will create R objects, with normalised counted using edgeR in the `Data` folder.  
A list of differential expressed genes base on Naive chromatin data was generated with the script in `Data > DE_more_stringent.R`. This list was then manually curated to remove potential false positive.

### Requirements for Step 2
Needed external softwares:
* R (edgeR, biomaRT)

## Step 3: Get last 5kb
This step aimed at at removing potential bias due to partially trasncribed chromatin associated transcripts (i.e. not yet ready to be exported towards the nucleoplasm):
* Identify expressed cytoplasmic transcripts with cufflinks see `Isoforms > cufflinks.sh` for exact options. Cufflinks results are given in the various sub folders under `Isoforms`. All these individual results were aggregated for all samples and filtered to keep only genes in the list of inducible LPA genes, defined in previous step. 
* Manually curate TSS, TES (compared to gencode annotations) and expressed transcripts (compared to cufflinks output) see `Manual Curation`
* To compare manually curated TSS and TES to external database see `Comparison with TSS-TES DB > TSS_polyA_distance_for_table.R` folder, the relevant information was extracted from the manual curation as `Comparison with TSS-TES DB > Gene_for_polyA_TSS_distance.txt`. While bed files from the `Comparison with TSS-TES DB > polyAsite_atlas.cluster.mm10.2-0.bed.gz` was dowloaded from PolyASite[https://polyasite.unibas.ch/atlas#3] and `Comparison with TSS-TES DB > refTSS_v3-1_mouse_coordinate.mm10.bed.gz` was download from RefTSS[http://reftss.clst.riken.jp/reftss/Main_Page]. The r script must be run from within its parent folder.
* Then extract gtf corresponding to the last 5kb to get counts see `Last_5kb` folder. The relevant information was extracted from the manual curation as `Last_5kb > gene_list_final_lpa.txt`.

export annotation and counts and generated rpkm/cpm

### Requiremetnts for Step3
Needed external softwares:
* cufflinks
* Python 3.7 (glob, gzip)
* IGV (for manual curation of TSS and TES)
* featuresCounts
* R (ggplot2, gridExtra, grid, scales)




-- To continue


create dataset 5kb

Figure: PCA DEGs on merged counts

