# mRNA-nuclear-export

## Step 1: RNAseq processing and QC
All files/scripts related to this step are listed under the `RNAseq processing pipeline` folder.  
RNAseq data were processed from qseq to counts using the pipeline `pipeline_ENCODE_ref.sh` ran on a Linux server ubuntu 16.04. The ouptut of this step are count files which are made available in the `Data > merged_counts` folder. We recommend starting after this step.  
For this step multiple scripts were used for each processing substeps:
* Conversion of qseq to fastq files see bash script`02_consolidated_michael.sh`
* Trimming adapter sequence using cudadapt, to see exact options used see `03_trimming_index_universal.sh`. Fastq files obtained after this step were deposited on ENCODE DCC
* Reads were aligned using STAR software, to see exact options used see `05_trimming_index_universal.sh`
* Alignments were filtered to keep only reads mapped in proper pair and remove unmapped read/mates or failing vendor quality using samtools, to see exact options used see `06_filtering.sh`. 
* Alignment were filtered to only keep uniquely aligned reads using samtools. To see exact options used see `06_filtering_unique.sh`.
* Optional. Create bigwig track files to visualize sequencing with IGV using RSeQC `bamtowig.py` and USCS `wigToBigWig` tools, for exact options see `07_tracks.sh` and `07_tracks_unique.sh`
* Counts were generated using featureCounts. To see exact options used see `08_counts.sh`. Count files obtained from this step are available in the folder `Data > individual_counts`. 
* QC of the various steps was visualized using multiQC. Additionaly PCA using R was also done on all the technical replicates, see `QC > PCA.R`
* Bam files corresponding to technical replicates were merged into a single bam file using samtools, see `merging_bam.sh` for exact options. Bams files generated by this step were deposited on ENCODE. Counts corresponding to merged bam files were generated using featureCounts. To see exact options used see `merged_counts.sh`. Count files obtained from this step are available in the folder `Data > merged_counts`. 

### Requirements for Step 1
Needed external softwares:
* python
* cutadapt
* STAR
* samtools
* RSeQC
* wigToBigWig
* multiQC
* R (ggplot2, edgeR)

## Step 2: Get List of early Lipid A inducible genes
This step starts by creating R datasets of rpkm from the raw counts in `Data > merged_counts` using the script `Data > Create Datasets.R`.
This script must be run from within the `Data` folder. This will create R objects, with normalised counted using edgeR in the `Data` folder.  
A list of differential expressed genes base on Naive chromatin data was generated with the script in `Data > DE_more_stringent.R`. This list was then manually curated to remove potential false positive.

### Requirements for Step 2
Needed external softwares:
* R (edgeR, biomaRT)

## Step 3: Get last 5kb
This step aimed at at removing potential bias due to partially trasncribed chromatin associated transcripts (i.e. not yet ready to be exported towards the nucleoplasm):
* Identify expressed cytoplasmic transcripts with cufflinks see `Isoforms > cufflinks.sh` for exact options. Cufflinks results are given in the various sub folders under `Isoforms`. All these individual results were aggregated for all samples and filtered to keep only genes in the list of inducible LPA genes, defined in previous step. 
* Manually curate TSS, TES (compared to gencode annotations) and expressed transcripts (compared to cufflinks output) see `Manual Curation`
* To compare manually curated TSS and TES to external database see `Comparison with TSS-TES DB > TSS_polyA_distance_for_table.R` folder, the relevant information was extracted from the manual curation as `Comparison with TSS-TES DB > Gene_for_polyA_TSS_distance.txt`. While bed files from the `Comparison with TSS-TES DB > polyAsite_atlas.cluster.mm10.2-0.bed.gz` was dowloaded from [PolyASite](https://polyasite.unibas.ch/atlas#3) and `Comparison with TSS-TES DB > refTSS_v3-1_mouse_coordinate.mm10.bed.gz` was download from [RefTSS](http://reftss.clst.riken.jp/reftss/Main_Page). The r script must be run from within its parent folder.
* Then extract gtf corresponding to the last 5kb to get counts see `Last_5kb` folder. The relevant information was extracted from the manual curation as `Last_5kb > gene_list_final_naive.txt` and  `Last_5kb > gene_list_final_lpa.txt`. The annotations ([gencode.vM14.gtf](https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M14/gencode.vM14.annotation.gtf.gz)) were filtered to include only the last 5kp of the relevant genes based on those list and these new subsetted annotation were used to get counts of sequencing reads falling with those regions using featureCounts, see `Last_5kb > run_gtf5kb.sh` and `Last_5kb > get_5kb_gtf_with_dedup.py`. The resulting count files are given in `Data > merged_counts`. 
* R datasets based on those last 5kb read were generated using the script 'Data > Create Datasets Last 5kb.R` which must be run from within the `Data` folder.

### Requirements for Step 3
Needed external softwares:
* cufflinks
* Python 3.7 (glob, gzip, re, argparse)
* IGV (for manual curation of TSS and TES)
* featuresCounts
* R (ggplot2, gridExtra, grid, scales, edgeR, biomaRT)

## Step 4: Model fitting
Fitting can be done by running the script `Modeling > Main_weighting_for_smoothing_include_negbinom_error_in_model_reps.R` from the `Modeling` folder with this command `Rscript Main_weighting_for_smoothing_include_negbinom_error_in_model_reps.R i ri cond folder rep` where `i` is the gene index to be fitted, `ri` is the number of random initialisation to do, `cond` is the condition to use,  `folder` is the output folder for the results, `rep` is the replicate to fit.   
For example: `Rscript Main_weighting_for_smoothing_include_negbinom_error_in_model_reps.R 1 1000 naive Results rep1`
All the results obtianed from this step are located under `Modeling > Results`.

### Requirements for Step 4
Needed external softwares:
* R (compiler, deSolve)

## Step 5: Sensitivity analysis
Sensitivity Analysis can be done by running the script `Modeling > Main_sa_dMod_approximate.R` for the approximate method and `Modeling > Main_sa_dMod_approximate.R` for the exact method from the `Modeling` folder with this command `Rscript Main_sa_dMod_approximate.R i cond folder rep` where `i` is the gene index to be fitted, `cond` is the condition to use, `folder` is the output folder for the results, `rep` is the replicate to fit.   
For example: `Rscript Main_sa_dMod_approximate 1 naive Results rep1`
All the results obtianed from this step are located under `Modeling > Results`.

### Requirements for Step 5
Needed external softwares:
* R (compiler, deSolve, dMod, numDeriv)

## Step 6: Intron retention
Intron retenttion was calculated on nucleoplasmic fraction bam files using [SQUID](https://github.com/Xinglab/SQUID), for exact options using to run SQUID see `Intron retention > squid_run.sh`. Results are given in `Intron retention > SQUID` folder.    
The R script `Intron retention > Introns.R` must be run from its parent directory.

### Requirements for Step 6
Needed external softwares:
* python 2.7 (NumPy, SciPy, pysam) for SQUID
* R (biomaRt)

## Step 7: Half-life estimation from ActinomycinD treated samples
Half-life estimates from ActinomycinD RNAseq were derived using the [ActDAnalyser](https://github.com/signalingsystemslab/ActDAnalyser) and resulting estimates can be found in the `ActD` folder. Fastqs and bams files used to derive those half-life have been deposited on SRA.

### Requirements for Step 7:
Needed external softwares:
* R (ggplot2, stats, ggpubr, gridExtra, edgeR)

## Step 10: Figures
Script to generate most figures in the manuscript and some supplementary documents is found under `Figures > Manuscript_figures.R` similarly to previously this script need to be run under its parent directory.

### Requirements for Step 10:
Needed external softwares:
* R (RColorBrewer, grid, ggplot2, openxlsx, biomaRt, compiler, scales, deSolve, ComplexHeatmap, circlize, MASS, pheatmap, gridExtra, plotly)

-- To continue
## Step 8: ChIPseq
Histone ChIPseq fastq files were deposited on ENCODE DCC and processed using [Encode histone ChIPseq pipeline](https://github.com/ENCODE-DCC/chip-seq-pipeline2) and annotated to the closest gene with HOMER, resulting files are located in `ChIPSeq > Results`. Then machine learning model was developped to see if histone marks could be a factor influencing effective transport rate, see `ChIPSeq > caret.R` this script must be run from within it parent directory.

### Requirements for Step 8:
Needed external softwares:
* Python3 (caper)
* HOMER
* R (data.table, dplyr, ggplot2, leaps, gridExtra, caret, skimr, RANN, doSNOW, plyr, grid, xgboost, pheatmap, svglite, alr3)

!!! Need to find script to make csv from modeling results for Kevin's script inputs: 
* parameter.csv


## Step 9: RBP

### Requirements for Step 9:
Needed external softwares:
* HOMER
* R (ggplot2, stats, ggpubr, gridExtra, edgeR)
